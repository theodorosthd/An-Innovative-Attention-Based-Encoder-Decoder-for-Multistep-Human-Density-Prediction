{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_linear_MultiStepForecast_vor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWwWQE-os1mm"
      },
      "source": [
        "#Mount Drive and basic imports \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdpri5p6DotE",
        "outputId": "ac6e3ae3-4f61-4c16-e874-eeef43b963a1"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKJg9jcKEPnV"
      },
      "source": [
        "# Load Dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "L4PDYvTaD2Lv",
        "outputId": "0dd1352d-7901-42ae-9d23-ccc6b4322fe6"
      },
      "source": [
        "PATH = ''\n",
        "df = pd.read_csv(PATH)\n",
        "df.columns =['timestamp', 'region_1', 'region_2', 'region_3','region_4','region_5','region_6']\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>region_1</th>\n",
              "      <th>region_2</th>\n",
              "      <th>region_3</th>\n",
              "      <th>region_4</th>\n",
              "      <th>region_5</th>\n",
              "      <th>region_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>184.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>89.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>358.0</td>\n",
              "      <td>196.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>176.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>469.0</td>\n",
              "      <td>451.0</td>\n",
              "      <td>334.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>246.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>574.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>445.0</td>\n",
              "      <td>460.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>307.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>608.0</td>\n",
              "      <td>628.0</td>\n",
              "      <td>490.0</td>\n",
              "      <td>533.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>348.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   timestamp  region_1  region_2  region_3  region_4  region_5  region_6\n",
              "0        1.0     154.0     184.0     107.0     119.0      94.0      89.0\n",
              "1        2.0     322.0     358.0     196.0     236.0     147.0     176.0\n",
              "2        3.0     469.0     451.0     334.0     375.0     165.0     246.0\n",
              "3        4.0     574.0     567.0     445.0     460.0     190.0     307.0\n",
              "4        5.0     608.0     628.0     490.0     533.0     240.0     348.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--QYcikOs-x0"
      },
      "source": [
        "# Import linear algorithms from sklearn \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHGCk7Rs2Fvb"
      },
      "source": [
        "# spot check nonlinear algorithms\n",
        "from numpy import load\n",
        "from numpy import loadtxt\n",
        "from numpy import nan\n",
        "from numpy import isnan\n",
        "from numpy import count_nonzero\n",
        "from numpy import unique\n",
        "from numpy import array\n",
        "from sklearn.base import clone\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.tree import ExtraTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHbS3Kcm8QC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23256712-68e8-4223-cc48-4dc6e7f2ed2e"
      },
      "source": [
        "# load data and split into chunks\n",
        "from numpy import unique\n",
        " \n",
        "# split the dataset by 'chunkID', return a dict of id to rows\n",
        "#chunk_ix=0 -> each day is a chunk \n",
        "def to_chunks(values, chunk_ix=0):\n",
        "\tchunks = dict()\n",
        "\t# get the unique chunk ids\n",
        "\tchunk_ids = unique(values[:, chunk_ix])\n",
        "\t# group rows by chunk id\n",
        "\tfor chunk_id in chunk_ids:\n",
        "\t\tselection = values[:, chunk_ix] == chunk_id\n",
        "\t\tchunks[chunk_id] = values[selection, :]\n",
        "\treturn chunks\n",
        " \n",
        "# load dataset\n",
        "\n",
        "# group data by chunks\n",
        "values = df_total.values\n",
        "chunks = to_chunks(values)\n",
        "print('Total Chunks: %d' % len(chunks))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Chunks: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDh5SfSEYNs-"
      },
      "source": [
        "# Splitting dataset into chunks (validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u-KdWDhxM0e",
        "outputId": "0e5c15d7-f878-4d53-85f4-dd254b9bc566"
      },
      "source": [
        "df_total.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8418, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQdmPa8qxFMe",
        "outputId": "a1b87ff4-ce09-4a6c-8a8b-8b0bb1022cb4"
      },
      "source": [
        "\n",
        "# split into standard hours\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        " \n",
        "# split a univariate dataset into train/test sets\n",
        "def split_dataset(data):\n",
        "\t# split into standard weeks\n",
        "\ttrain, test = data[:1152], data[1152:]\n",
        "\t# restructure into windows of weekly data\n",
        "\ttrain = array(split(train, int(len(train)/6)))\n",
        "\ttest = array(split(test, len(test)/6))\n",
        "\treturn train, test\n",
        " \n",
        "# load the new file\n",
        "df2 = df[['region_1']]\n",
        "train, test = split_dataset(df2.values)\n",
        "# validate train data\n",
        "print(train.shape)\n",
        "print(train[0, 0, 0], train[-1, -1, 0])\n",
        "# validate test\n",
        "print(test.shape)\n",
        "print(test[0, 0, 0], test[-1, -1, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(192, 6, 1)\n",
            "154.0 0.0\n",
            "(144, 6, 1)\n",
            "0.0 582.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8KbUVut5Qx"
      },
      "source": [
        "# Direct Multistep Forecasting (6 steps ahead) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmt5_fQRBrep"
      },
      "source": [
        "from math import sqrt\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.linear_model import Lars\n",
        "from sklearn.linear_model import LassoLars\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "   \n",
        "# split a univariate dataset into train/test sets\n",
        "def split_dataset(data):\n",
        "\t# split into standard weeks\n",
        "\ttrain, test = data[:1152], data[1152:]\n",
        "\t# restructure into windows of weekly data\n",
        "\ttrain = array(split(train, int(len(train)/6)))\n",
        "\ttest = array(split(test, len(test)/6))\n",
        "\treturn train, test\n",
        " \n",
        "# evaluate one or more weekly forecasts against expected values\n",
        "def evaluate_forecasts(actual, predicted):\n",
        "\tscores = list()\n",
        "\tscores2 = list()\n",
        "\t# calculate an RMSE score for each day\n",
        "\tfor i in range(actual.shape[1]):\n",
        "\t\t# calculate mse\n",
        "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
        "\t\t# calculate rmse\n",
        "\t\trmse = sqrt(mse)\n",
        "    #calculate mae\n",
        "\n",
        "\t\tmae = mean_absolute_error(actual[:, i], predicted[:, i])\n",
        "\t\t# store\n",
        "\t\tscores.append(rmse)\n",
        "\t\tscores2.append(mae)\n",
        "\t# calculate overall RMSE\n",
        "\ts = 0\n",
        "\tfor row in range(actual.shape[0]):\n",
        "\t\tfor col in range(actual.shape[1]):\n",
        "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
        "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
        "\treturn score, scores, scores2\n",
        " \n",
        "# summarize scores\n",
        "def summarize_scores(name, score, scores, scores2):\n",
        "\ts_scores = ', '.join(['%.4f' % s for s in scores])\n",
        "\ts_scores2 = ', '.join(['%.4f' % s for s in scores2])\n",
        "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
        "\tprint('%s: %s' % (name, s_scores2))\n",
        " \n",
        "# prepare a list of ml models\n",
        "def get_models(models=dict()):\n",
        "\t# linear models\n",
        "\tmodels['lr'] = LinearRegression()\n",
        "\tmodels['lasso'] = Lasso() # DONE \n",
        "\tmodels['ridge'] = Ridge() # DONE \n",
        "\tmodels['en'] = ElasticNet() # DONE \n",
        "\tmodels['huber'] = HuberRegressor() # DONE \n",
        "\tmodels['lars'] = Lars()\n",
        "\tmodels['llars'] = LassoLars()\n",
        "\tmodels['pa'] = PassiveAggressiveRegressor(max_iter=20000, tol=1e-3) # DONE \n",
        "\tmodels['ranscac'] = RANSACRegressor()\n",
        "\tmodels['sgd'] = SGDRegressor(max_iter=20000, tol=1e-3) # DONE \n",
        "\tprint('Defined Direct %d models' % len(models))\n",
        "\treturn models\n",
        " \n",
        "# create a feature preparation pipeline for a model\n",
        "def make_pipeline(model):\n",
        "\tsteps = list()\n",
        "\t# standardization\n",
        "\tsteps.append(('standardize', StandardScaler()))\n",
        "\t# normalization\n",
        "\tsteps.append(('normalize', MinMaxScaler()))\n",
        "\t# the model\n",
        "\tsteps.append(('model', model))\n",
        "\t# create pipeline\n",
        "\tpipeline = Pipeline(steps=steps)\n",
        "\treturn pipeline\n",
        " \n",
        "# convert history into inputs and outputs\n",
        "def to_supervised(history, output_ix):\n",
        "\tX, y = list(), list()\n",
        "\t# step over the entire history one time step at a time\n",
        "\tfor i in range(len(history)-1):\n",
        "\t\tX.append(history[i][:,0])\n",
        "\t\ty.append(history[i + 1][output_ix,0])\n",
        "\treturn array(X), array(y)\n",
        " \n",
        "# fit a model and make a forecast\n",
        "def sklearn_predict(model, history):\n",
        "\tyhat_sequence = list()\n",
        "\t# fit a model for each forecast day\n",
        "\tfor i in range(6):\n",
        "\t\t# prepare data\n",
        "\t\ttrain_x, train_y = to_supervised(history, i)\n",
        "\t\t# make pipeline\n",
        "\t\tpipeline = make_pipeline(model)\n",
        "\t\t# fit the model\n",
        "\t\tpipeline.fit(train_x, train_y)\n",
        "\t\t# forecast\n",
        "\t\tx_input = array(train_x[-1, :]).reshape(1,6)\n",
        "\t\tyhat = pipeline.predict(x_input)[0]\n",
        "\t\t# store\n",
        "\t\tyhat_sequence.append(yhat)\n",
        "\treturn yhat_sequence\n",
        " \n",
        "# evaluate a single model\n",
        "def evaluate_model(model, train, test):\n",
        "\t# history is a list of weekly data\n",
        "\thistory = [x for x in train]\n",
        "\t# walk-forward validation over each week\n",
        "\tpredictions = list()\n",
        "\tfor i in range(len(test)):\n",
        "\t\t# predict the week\n",
        "\t\tyhat_sequence = sklearn_predict(model, history)\n",
        "\t\t# store the predictions\n",
        "\t\tpredictions.append(yhat_sequence)\n",
        "\t\t# get real observation and add to history for predicting the next week\n",
        "\t\thistory.append(test[i, :])\n",
        "\tpredictions = array(predictions)\n",
        "\t# evaluate predictions days for each week\n",
        "\tscore, scores, scores2 = evaluate_forecasts(test[:, :, 0], predictions)\n",
        "\treturn score, scores, scores2\n",
        " \n",
        "\n",
        "df2 = df[['region_3']]\n",
        "# split into train and test\n",
        "train, test = split_dataset(df2.values)\n",
        "# prepare the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate each model\n",
        "\n",
        "for name, model in models.items():\n",
        "\t# evaluate and get scores\n",
        "\tscore, scores, scores2 = evaluate_model(model, train, test)\n",
        "\t# summarize scores\n",
        "\tsummarize_scores(name, score, scores, scores2)\n",
        "\t# plot scores\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG9YL-STuHo8"
      },
      "source": [
        "# Recursive Multistep Forecasting (6 steps ahead) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkc2zyFlzGTB"
      },
      "source": [
        "# recursive multi-step forecast with linear algorithms\n",
        "from math import sqrt\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.linear_model import Lars\n",
        "from sklearn.linear_model import LassoLars\n",
        "from sklearn.linear_model import PassiveAggressiveRegressor\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        " \n",
        "# split a univariate dataset into train/test sets\n",
        "def split_dataset(data):\n",
        "\t# split into standard weeks\n",
        "\ttrain, test = data[:1152], data[1152:]\n",
        "\t# restructure into windows of weekly data\n",
        "\ttrain = array(split(train, int(len(train)/6)))\n",
        "\ttest = array(split(test, len(test)/6))\n",
        "\treturn train, test\n",
        "\n",
        " \n",
        "# evaluate one or more weekly forecasts against expected values\n",
        "def evaluate_forecasts(actual, predicted):\n",
        "\tscores = list()\n",
        "\tscores2 = list()\n",
        "\t# calculate an RMSE score for each day\n",
        "\tfor i in range(actual.shape[1]):\n",
        "\t\t# calculate mse\n",
        "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
        "\t\t# calculate rmse\n",
        "\t\trmse = sqrt(mse)\n",
        "    #calculate mae\n",
        "\n",
        "\t\tmae = mean_absolute_error(actual[:, i], predicted[:, i])\n",
        "\t\t# store\n",
        "\t\tscores.append(rmse)\n",
        "\t\tscores2.append(mae)\n",
        "\t# calculate overall RMSE\n",
        "\ts = 0\n",
        "\tfor row in range(actual.shape[0]):\n",
        "\t\tfor col in range(actual.shape[1]):\n",
        "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
        "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
        "\treturn score, scores, scores2\n",
        " \n",
        "# summarize scores\n",
        "def summarize_scores(name, score, scores, scores2):\n",
        "\ts_scores = ', '.join(['%.4f' % s for s in scores])\n",
        "\ts_scores2 = ', '.join(['%.4f' % s for s in scores2])\n",
        "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
        "\tprint('%s: %s' % (name, s_scores2))\n",
        " \n",
        "# prepare a list of ml models\n",
        "def get_models(models=dict()):\n",
        "\t# linear models\n",
        "\tmodels['lr'] = LinearRegression()\n",
        "\tmodels['lasso'] = Lasso()\n",
        "\tmodels['ridge'] = Ridge()\n",
        "\tmodels['en'] = ElasticNet()\n",
        "\tmodels['huber'] = HuberRegressor()\n",
        "\tmodels['lars'] = Lars()\n",
        "\tmodels['llars'] = LassoLars()\n",
        "\tmodels['pa'] = PassiveAggressiveRegressor(max_iter=20000, tol=1e-3)\n",
        "\tmodels['ranscac'] = RANSACRegressor()\n",
        "\tmodels['sgd'] = SGDRegressor(max_iter=20000, tol=1e-3)\n",
        "\tprint('Defined %d Recursive models' % len(models))\n",
        "\treturn models\n",
        " \n",
        "# create a feature preparation pipeline for a model\n",
        "def make_pipeline(model):\n",
        "\tsteps = list()\n",
        "\t# standardization\n",
        "\tsteps.append(('standardize', StandardScaler()))\n",
        "\t# normalization\n",
        "\tsteps.append(('normalize', MinMaxScaler()))\n",
        "\t# the model\n",
        "\tsteps.append(('model', model))\n",
        "\t# create pipeline\n",
        "\tpipeline = Pipeline(steps=steps)\n",
        "\treturn pipeline\n",
        " \n",
        "# make a recursive multi-step forecast\n",
        "def forecast(model, input_x, n_input):\n",
        "\tyhat_sequence = list()\n",
        "\tinput_data = [x for x in input_x]\n",
        "\tfor j in range(6):\n",
        "\t\t# prepare the input data\n",
        "\t\tX = array(input_data[-n_input:]).reshape(1, n_input)\n",
        "\t\t# make a one-step forecast\n",
        "\t\tyhat = model.predict(X)[0]\n",
        "\t\t# add to the result\n",
        "\t\tyhat_sequence.append(yhat)\n",
        "\t\t# add the prediction to the input\n",
        "\t\tinput_data.append(yhat)\n",
        "\treturn yhat_sequence\n",
        " \n",
        "# convert windows of weekly multivariate data into a series of total power\n",
        "def to_series(data):\n",
        "\t# extract just the total power from each week\n",
        "\tseries = [week[:, 0] for week in data]\n",
        "\t# flatten into a single series\n",
        "\tseries = array(series).flatten()\n",
        "\treturn series\n",
        " \n",
        "# convert history into inputs and outputs\n",
        "def to_supervised(history, n_input):\n",
        "\t# convert history to a univariate series\n",
        "\tdata = to_series(history)\n",
        "\tX, y = list(), list()\n",
        "\tix_start = 0\n",
        "\t# step over the entire history one time step at a time\n",
        "\tfor i in range(len(data)):\n",
        "\t\t# define the end of the input sequence\n",
        "\t\tix_end = ix_start + n_input\n",
        "\t\t# ensure we have enough data for this instance\n",
        "\t\tif ix_end < len(data):\n",
        "\t\t\tX.append(data[ix_start:ix_end])\n",
        "\t\t\ty.append(data[ix_end])\n",
        "\t\t# move along one time step\n",
        "\t\tix_start += 1\n",
        "\treturn array(X), array(y)\n",
        " \n",
        "# fit a model and make a forecast\n",
        "def sklearn_predict(model, history, n_input):\n",
        "\t# prepare data\n",
        "\ttrain_x, train_y = to_supervised(history, n_input)\n",
        "\t# make pipeline\n",
        "\tpipeline = make_pipeline(model)\n",
        "\t# fit the model\n",
        "\tpipeline.fit(train_x, train_y)\n",
        "\t# predict the week, recursively\n",
        "\tyhat_sequence = forecast(pipeline, train_x[-1, :], n_input)\n",
        "\treturn yhat_sequence\n",
        " \n",
        "# evaluate a single model\n",
        "def evaluate_model(model, train, test, n_input):\n",
        "\t# history is a list of hourly data\n",
        "\thistory = [x for x in train]\n",
        "\t# walk-forward validation over each hour\n",
        "\tpredictions = list()\n",
        "\tfor i in range(len(test)):\n",
        "\t\t# predict the hour\n",
        "\t\tyhat_sequence = sklearn_predict(model, history, n_input)\n",
        "\t\t# store the predictions\n",
        "\t\tpredictions.append(yhat_sequence)\n",
        "\t\t# get real observation and add to history for predicting the next hour\n",
        "\t\thistory.append(test[i, :])\n",
        "\tpredictions = array(predictions)\n",
        "\t# evaluate predictions days for each hour\n",
        "\tscore, scores, scores2 = evaluate_forecasts(test[:, :, 0], predictions)\n",
        "\treturn score, scores, scores2\n",
        " \n",
        "df2 = df[['region_1']]\n",
        "train, test = split_dataset(df2.values)\n",
        "# prepare the models to evaluate\n",
        "models = get_models()\n",
        "n_input = 3 #prior timestamp observations\n",
        "# evaluate each model\n",
        "\n",
        "for name, model in models.items():\n",
        "\t# evaluate and get scores\n",
        "\tscore, scores, scores2 = evaluate_model(model, train, test, n_input)\n",
        "\t# summarize scores\n",
        "\tsummarize_scores(name, score, scores, scores2)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}